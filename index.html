<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xikai Tang</title>
  
  <meta name="author" content="Xikai Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xikai Tang</name>
              </p>
              <p> 
		I'm currently a Ph.D student in Software Engineering from <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China</a>. In 2023, I obtained my MASc in the Department of Electrical and Computer Engineering, <a href="https://uwaterloo.ca/"> University of Waterloo</a>, advised by Prof. <a
                href="https://ece.uwaterloo.ca/~dban/">Dayan Ban</a> and Prof. <a
                href="https://ece.uwaterloo.ca/~z70wang/">Zhou Wang</a>.
<!-- 		In 2023, I obtained my MASc in the Department of Electrical and Computer Engineering, <a href="https://uwaterloo.ca/"> University of Waterloo</a>, advised by Prof. <a
                href="https://ece.uwaterloo.ca/~dban/">Dayan Ban</a> and Prof. <a
                href="https://ece.uwaterloo.ca/~z70wang/">Zhou Wang</a>. -->
              </p>
              <p>
              I am broadly interested in computer vision and deep learning. My current research focuses on 3D vision, representation learning.
              </p>
              <p style="text-align:center">
<!--                 <a href="mailto:wziyi22@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp -->
		<a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=mcVaJMUAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Jasper-kk"> Github </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/txk.jpg">
            </td>
          </tr>
        </tbody></table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	      <li style="margin: 5px;" >
                <b>2024-01:</b> The journal paper of <a href="https://arxiv.org/abs/2208.02812">P2P</a> is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI 2024</a>.
              </li>
	      <li style="margin: 5px;" >
                <b>2023-07:</b> 1 paper on 3D generative pre-training is accepted to <a href="https://iccv2023.thecvf.com">ICCV 2023</a>.
              </li>
	      <li style="margin: 5px;" >
                <b>2023-07:</b> The journal paper of <a href="https://arxiv.org/abs/2012.00987">PV-RAFT</a> is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI 2023</a>.
              </li>
	      <li style="margin: 5px;" >
                <b>2022-09:</b> 1 paper (spotlight) on 3D prompt learning is accepted to <a href="https://neurips.cc/Conferences/2022">NeurIPS 2022</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2022-03:</b> 1 paper on 3D semantic segmentation is accepted to <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-07:</b> 2 papers (including 1 oral) are accepted to <a href="http://iccv2021.thecvf.com/">ICCV 2021</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2021-03:</b> 1 paper on 3D scene flow estimation is accepted to <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.
              </li>
            </p>
          </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	    <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pointa.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Point-A: Memory Channel Attention-based Network for 3D Point Clouds</papertitle>
              <br>
<!--               Fangzheng Huang,
              <strong>Xikai Tang</strong>,
	      Chao Li,
              <a href="https://ece.uwaterloo.ca/~dban/">Dayan Ban</a>,  -->
              <br>
              <em>coming soon, 2024
              <br>
<!--               <a href="https://www.sciencedirect.com/science/article/abs/pii/S1568494623011146">[Paper]</a>
              <br> -->
<!--               <p> P2P++ is the extended journal version of <a href="https://arxiv.org/abs/2208.02812">P2P</a>. We further propose Pixel-to-Point Distillation to make P2P applicable in scene-level perception tasks. </p>
            </td> -->
          </tr>

	    <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/mi3d.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Semantic 2D-3D data under Multi-Illuminance for Indoor Scene Understanding	</papertitle>
              <br>
<!--               Fangzheng Huang,
              <strong>Xikai Tang</strong>,
	      Chao Li,
              <a href="https://ece.uwaterloo.ca/~dban/">Dayan Ban</a>,  -->
              <br>
              <em>coming soon, 2024
              <br>
<!--               <a href="https://www.sciencedirect.com/science/article/abs/pii/S1568494623011146">[Paper]</a>
              <br> -->
<!--               <p> P2P++ is the extended journal version of <a href="https://arxiv.org/abs/2208.02812">P2P</a>. We further propose Pixel-to-Point Distillation to make P2P applicable in scene-level perception tasks. </p>
            </td> -->
          </tr>
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Cyclicstylega.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Cyclic style generative adversarial network for near infrared and visible light face recognition</papertitle>
              <br>
              Fangzheng Huang,
              <strong>Xikai Tang</strong>,
	      Chao Li,
              <a href="https://ece.uwaterloo.ca/~dban/">Dayan Ban</a>, 
              <br>
              <em>Applied Soft Computing, 2024
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S1568494623011146">[Paper]</a>
              <br>
<!--               <p> P2P++ is the extended journal version of <a href="https://arxiv.org/abs/2208.02812">P2P</a>. We further propose Pixel-to-Point Distillation to make P2P applicable in scene-level perception tasks. </p>
            </td> -->
          </tr>
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/scanet.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SCA-Net: Spatial and channel attention-based network for 3D point clouds</papertitle>
              <br>
              <strong>Xikai Tang</strong>*,
              Karim Habashy,
              Fangzheng Huang
	      Chao Li,
              <a href="https://ece.uwaterloo.ca/~dban/">Dayan Ban</a>, 
              <br>
              <em>Computer Vision and Image Understanding, 2023
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S107731422300070X">[Paper]</a>
<!--               <a href="https://github.com/weiyithu/PV-RAFT">[Code]</a>
              <a href="https://pvraft.ivg-research.xyz">[Project Page]</a>  -->
              <br>
<!--               <p> DPV-RAFT is the extended journal version of <a href="https://arxiv.org/abs/2012.00987">PV-RAFT</a>. We further propose Spatial Deformation and Temporal Deformation to enhance PV-RAFT. </p>
            </td> -->
          </tr>

	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/surveysmall.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>A survey on end-to-end point cloud learning</papertitle>
              <br>
              <strong>Xikai Tang</strong>*,
              Fangzheng Huang
	      Chao Li,
              <a href="https://ece.uwaterloo.ca/~dban/">Dayan Ban</a>, 
              <br>
              <em>IET Image Processing, 2023
              <br>
              <a href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ipr2.12729">[Paper]</a>
<!--               <a href="https://github.com/wangzy22/TAP">[Code]</a>
              <a href="http://tap.ivg-research.xyz">[Project Page]</a>  -->
              <br>
<!--               <p> TAP is a 3D-to-2D generative pre-training method that generate projected images of point clouds from instructed perspectives. </p>
            </td> -->
          </tr>
		
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Nearsurvey.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Near-infrared and visible light face recognition: a comprehensive survey</papertitle>
              <br>
              Fangzheng Huang,
              <strong>Xikai Tang</strong>,
	      Chao Li,
              <a href="https://ece.uwaterloo.ca/~dban/">Dayan Ban</a>, 
              <br>
              <em>Soft Computing, 2023
              <br>
              <a href="https://link.springer.com/article/10.1007/s00500-023-08366-8">[Paper]</a>
<!--               <a href="https://github.com/wangzy22/P2P">[Code]</a>
              <a href="https://p2p.ivg-research.xyz/">[Project Page]</a> 
              <a href="https://zhuanlan.zhihu.com/p/558286235">[中文解读]</a>  -->
              <br>
<!--               <p> P2P is a framework to leverage large-scale pre-trained image models for 3D point cloud analysis. </p>
            </td> -->
          </tr>
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/FCT.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Pedestrian Attribute Recognition with Feature Combination in Transformer with Attention Model</papertitle>
              <br>
              <strong>Xikai Tang</strong>,
              Zhikun Lin,
              Yiran Wang,
              <br>
              <em>6th International Technical Conference on Advances in Computing, Control and Industrial Engineering (CCIE 2021), 2021
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-981-19-3927-3_49">[Paper]</a> 
<!--               <a href="https://github.com/wangzy22/SemAffiNet">[Code]</a> 
              <br> -->
<!--               <p> We present Semantic-Affine Transformation that transforms decoder mid-level features of the encoder-decoder segmentation network with class-specific affine parameters.</p>
            </td> -->
          </tr>

<!--           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PoinTr.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers</papertitle>
              <br>
              <a href="https://yuxumin.github.io/">Xumin Yu</a>*,  
              <a href="https://raoyongming.github.io">Yongming Rao</a>*,
              <strong>Ziyi Wang</strong>, Zuyan Liu, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>

              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2108.08839">[arXiv]</a> 
              <a href="https://github.com/yuxumin/PoinTr/">[Code]</a> 
              <a href="https://zhuanlan.zhihu.com/p/401928647">[中文解读]</a>
              <br>
              <p> PoinTr is a transformer-based framework that reformulates point cloud completion as a set-to-set translation problem. </p>
            </td>
          </tr> -->

<!--           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DIML.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle> Towards Interpretable Deep Metric Learning with Structural Matching
              </papertitle>
              <br>
              <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>*, 
              <a href="https://raoyongming.github.io">Yongming Rao</a>*,
              <strong>Zyi Wang</strong>,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2108.05889">[arXiv]</a> <a href="https://github.com/wl-zhao/DIML">[Code]</a> 
              <br>
              <p> We present a deep interpretable metric learning (DIML) that adopts a structural matching strategy to explicitly aligns the spatial embeddings by computing an optimal matching flow between feature maps of the two images. </p>
            </td>
          </tr> -->

<!--           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PV_RAFT.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds</papertitle>
              <br>
              <a href="https://weiyithu.github.io"> Yi Wei </a>*, 
              <strong>Ziyi Wang*</strong>, 
              <a href="https://raoyongming.github.io">Yongming Rao</a>*,
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2012.00987">[arXiv]</a> <a href="https://github.com/weiyithu/PV-RAFT">[Code]</a>
              <br>
              <p></p>
              <p> We present point-voxel correlation fields for 3D scene flow estimation which migrates the high performance of RAFT and provides a solution to build structured all-pairs correlation fields for unstructured point clouds. </p>
            </td>
          </tr> -->

<!-- 	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              <p>
		<li style="margin: 5px;"> Teaching Assistant, Computer Vision, 2024 Spring Semester</li>
		<li style="margin: 5px;"> Teaching Assistant, Pattern Recognition and Machine Learning, 2022 Fall Semester</li>
              </p>
            </td>
          </tr>
        </tbody></table> -->

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
		<li style="margin: 5px;"> 2023 ChangXin Memory Scholarship, Tsinghua University</li>
		<li style="margin: 5px;"> 2023 CVPR Outstanding Reviewer</li>
                <li style="margin: 5px;"> 2021 Haining Talent Scholarship, Tsinghua University</li>
                <li style="margin: 5px;"> 2020 Excellent graduation thesis, Tsinghua University</li>
                <li style="margin: 5px;"> 2018 Zheng Geru Scholarship, Tsinghua University</li>
                <li style="margin: 5px;"> 2017 Hongqian Electronics Scholarship, Tsinghua University</li>
              </p>
            </td>
          </tr>
        </tbody></table> -->
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=9CxOMjcpU9w-plJyTUCLdeFnIgwW-GgMgaWu0l1B-xk"></script>
	  </div>        
	  <br>
	    &copy; Xikai Tang | Last updated: May 20, 2024
</center></p>
</body>

</html>
